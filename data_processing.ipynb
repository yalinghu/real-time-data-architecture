{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0be9d549",
   "metadata": {},
   "source": [
    "# Prerrequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec49f03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/jovyan/spark-3.0.2-bin-hadoop3.2\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages io.delta:delta-core_2.12:0.8.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.2 --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ae3d3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0210403",
   "metadata": {},
   "source": [
    "# Set up Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aba4500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://c6a2a05ee792:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Challenge</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f792c63d820>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Challenge\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d3e065",
   "metadata": {},
   "source": [
    "# Connection to Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bee99b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "schema_customers= StructType([\n",
    "    StructField('id', StringType(), True),\n",
    "    StructField('first_name', StringType(), True),\n",
    "    StructField('last_name', StringType(), True),\n",
    "    StructField('email', StringType(), True),\n",
    "    StructField('__deleted', StringType(), True)\n",
    "])\n",
    "\n",
    "Schema= StructType([\n",
    "                StructField('schema', StringType()),\n",
    "                StructField('payload', schema_customers)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f183248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- parsed_value: struct (nullable = true)\n",
      " |    |-- schema: string (nullable = true)\n",
      " |    |-- payload: struct (nullable = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- first_name: string (nullable = true)\n",
      " |    |    |-- last_name: string (nullable = true)\n",
      " |    |    |-- email: string (nullable = true)\n",
      " |    |    |-- __deleted: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_stream = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "  .option(\"subscribe\", \"dbserver1.inventory.customers\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .load() \\\n",
    "  .select(f.from_json(f.col(\"value\").cast(\"string\"), Schema).alias(\"parsed_value\"))\n",
    "  \n",
    "\n",
    "print(df_stream.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4afdc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/12 08:22:50 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a896a585-3aaf-4e88-b8e2-e3fbbda1c189. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f792c62dfa0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stream.select(\"parsed_value.payload.*\").writeStream\\\n",
    "    .outputMode(\"update\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .queryName(\"Customers_query\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c38487",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM Customers_query\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4cc1e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE customers\n",
    "(id int, first_name string, last_name string, email string, __deleted boolean)\n",
    "USING delta\n",
    "LOCATION 'hdfs://namenode:9000/*****'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9f6933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "deltaTable_customers = DeltaTable.forPath(spark, \"hdfs://namenode:9000/*****\")\n",
    "\n",
    "# Function to upsert microBatchOutputDF into Delta table using merge\n",
    "def upsertCustomersToDelta(microBatchOutputDF, batchId):\n",
    "  deltaTable_customers.alias(\"t\").merge(\n",
    "      microBatchOutputDF.alias(\"s\"),\n",
    "      \"s.id = t.id\") \\\n",
    "      .whenMatchedDelete(condition = \"s.__deleted = 'true'\") \\\n",
    "      .whenMatchedUpdate(set = { \"id\": \"s.id\", \"first_name\": \"s.first_name\", \"last_name\": \"s.last_name\", \"email\": \"s.email\" } ) \\\n",
    "      .whenNotMatchedInsert(values = {\"id\": \"s.id\", \"first_name\": \"s.first_name\", \"last_name\": \"s.last_name\", \"email\": \"s.email\"}) \\\n",
    "      .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56e42ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stream.select(\"parsed_value.payload.*\").writeStream\\\n",
    "    .format(\"delta\") \\\n",
    "    .foreachBatch(upsertCustomersToDelta) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\",\"hdfs://namenode:9000/*****\") \\\n",
    "    .start(\"hdfs://namenode:9000/*****\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77655a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
